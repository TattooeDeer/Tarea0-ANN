{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  jQuery(document).ready(function($) {\n",
    "\n",
    "  $(window).load(function(){\n",
    "    $('#preloader').fadeOut('slow',function(){$(this).remove();});\n",
    "  });\n",
    "\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<style type=\"text/css\">\n",
    "  div#preloader { position: fixed;\n",
    "      left: 0;\n",
    "      top: 0;\n",
    "      z-index: 999;\n",
    "      width: 100%;\n",
    "      height: 100%;\n",
    "      overflow: visible;\n",
    "      background: #fff url('http://preloaders.net/preloaders/720/Moving%20line.gif') no-repeat center center;\n",
    "  }\n",
    "\n",
    "</style>\n",
    "\n",
    "<div id=\"preloader\"></div>\n",
    "\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/escudo_utfsm.gif\" style=\"float:right;height:100px\">\n",
    "<img src=\"images/IsotipoDIisocolor.png\" style=\"float:left;height:100px\">\n",
    "<center>\n",
    "\n",
    "    <i><h2 style=\"font-family:serif;font-size:300%; text-align:center;color:#283A5B\"> Tarea 0</h2></i>\n",
    "    <h1 style=\"font-family:serif;font-size:200%; text-align:center;color:#4d4d4d\"> <i> Redes Neuronales Artificiales - San Joaquín </i></h1>\n",
    "    <h3 style = 'font-family:serif;font-size:120%'><i> Ignacio Loayza C. 201273604-8</i></h3>\n",
    "\n",
    "</center>\n",
    "\n",
    "<p>\n",
    "<center>_Marzo 2018_ </center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "p {\n",
       "    color: black;\n",
       "    font-family: Serif;\n",
       "    font-size: 1.23em;\n",
       "}\n",
       ".output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "%matplotlib inline\n",
    "\n",
    "# semilla\n",
    "seed = hash(\"Ñanculef es mi pastor, nada me ha de faltar\")%2^32\n",
    "\n",
    "# Imports\n",
    "\n",
    "#Neural Networks\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "#numerico\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Machine Learning\n",
    "\n",
    "#Metricas\n",
    "\n",
    "#Graficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set_style(\"darkgrid\")\n",
    "import graphviz\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Shallow copy\n",
    "import copy\n",
    "\n",
    "#preprocesamiento\n",
    "\n",
    "\n",
    "#HTML incrustation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "p {\n",
    "    color: black;\n",
    "    font-family: Serif;\n",
    "    font-size: 1.23em;\n",
    "}\n",
    ".output_png {\n",
    "        display: table-cell;\n",
    "        text-align: center;\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "</style>\n",
    "\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><h2 style=\"font-family:serif;font-size:260%; text-align:center;color:#004d80\"> Back-propagation from Scratch</h2></i>\n",
    "\n",
    "<i><h3 style=\"font-family:serif;font-size:200%; text-align:left;color:#4d4d4d\"> I) Entrenando una red FF mediante Back-propagation</h3></i>\n",
    "\n",
    "Comenzaremos entrenando una red del tipo *Feed Forward* con arquitectura de dos capas ocultas, distribuidas de la forma  32:16 neuronas en las capas y *K* neuronas de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clases necesarias\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, n_inputs, neurons_each_layer, n_outputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.neurons_each_layer = neurons_each_layer\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        # Inicializacion de las capas\n",
    "        layers_array = []\n",
    "        index = 0\n",
    "        n_neurons_prev_layer = 0 # para la capa de input\n",
    "\n",
    "        #input layer\n",
    "        layers_array.append(Layer(n_inputs, 0, index))\n",
    "        n_neurons_prev_layer = n_inputs\n",
    "\n",
    "        #Hidden layers\n",
    "        for layer_conf in neurons_each_layer:\n",
    "            index+=1\n",
    "            layers_array.append(Layer(layer_conf, n_neurons_prev_layer,index))\n",
    "            n_neurons_prev_layer = layer_conf\n",
    "            \n",
    "        #Output layer\n",
    "        layers_array.append(Layer(n_outputs, n_neurons_prev_layer, index))\n",
    "        self.hidden_layers = layers_array\n",
    "            \n",
    "        \n",
    "    def get_weights_matrix(layer_index):\n",
    "        return self.hidden_layers[layer_index].get_weights_matrix()\n",
    "    \n",
    "    def get_n_inputs(self):\n",
    "        return n_inputs\n",
    "    def get_neurons_each_layer(self):\n",
    "        return self.neurons_each_layer\n",
    "    def get_n_outputs(self):\n",
    "        return n_outputs\n",
    "\n",
    "    def get_network_parameters(self):\n",
    "        network_params = {'n_inputs':self.n_inputs,\n",
    "                        'neurons_each_layer':self.neurons_each_layer,\n",
    "                        'n_outputs':self.n_outputs,\n",
    "                         'hidden_layers':self.hidden_layers}\n",
    "        return network_params\n",
    "    \n",
    "    def forward_propagate(self, data_row):\n",
    "        previous_activations = np.ones((self.n_inputs,1))\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            if(layer.get_index() == 0):#Si es la primera capa le tenemos que pasar la data_column como las activaciones\n",
    "                layer.compute_layer_activations(data_row)\n",
    "                previous_activations = layer.get_activations_column()\n",
    "            else:\n",
    "                layer.compute_layer_activations(previous_activations)\n",
    "                previous_activations = layer.get_activations_column()\n",
    "                \n",
    "    def backward_pass(self, ):\n",
    "    \n",
    "    \n",
    "        # expected es un vector con las mismas dimensiones\n",
    "        #que la columna de activaciones de la capa de salida.\n",
    "        #Usemos el error cuadratico medio (MSE)\n",
    "    def calulate_loss(self, expected): \n",
    "        predicted = self.hidden_layers[-1].get_activations_column\n",
    "        return np.pow(expected - predicted,2)/2\n",
    "    def gradient_loss(self, expected):\n",
    "        predicted = self.hidden_layers[-1].get_activations_column\n",
    "        return predicted - expected\n",
    "                \n",
    "        \n",
    "    ##### UNDER CONSTRUCTION #################\n",
    "    def train(self, transfer_func, df_train, df_validate):\n",
    "        #Chequear si el numero de atributos concuerda con el numero de inputs especificados\n",
    "        if(len(df_train.columns) != self.n_inputs):\n",
    "            print('El número de columnas del dataset no coincide con el número de inputs especificado para esta red')\n",
    "            print(\"Número de columnas del dataset: %d\" %len(df_train.columns))\n",
    "            print(\"Número de inputs aceptados por la red: %d\" %self.n_inputs)\n",
    "            return\n",
    "        # extraer una fila y pasarsela a la funcion de forward_propagation\n",
    "        \n",
    "        \n",
    "        \n",
    "class Layer:\n",
    "    def __init__(self, neurons_in_layer, neurons_previous_layer, index, **kargs):\n",
    "        self.layer_index = index\n",
    "        self.neurons_in_layer = neurons_in_layer\n",
    "        self.activations_column = []\n",
    "        \n",
    "        #Funciones de transferencia disponibles para la capa\n",
    "        if('transfer_func' in kargs):\n",
    "            if(kargs['transfer_func'] == 'sigmoid'):\n",
    "                self.transfer = self.sigmoid\n",
    "                self.transfer_derivative = self.sigmoid_derivative\n",
    "            elif(kargs['transfer_func'] == 'softmax'):\n",
    "                self.transfer = self.softmax\n",
    "                self.transfer_derivative = self.softmax_derivative\n",
    "            elif(kargs['transfer_func'] == 'lineal' or index == 0): #Si es la capa de inputs le damos transferencia lineal\n",
    "                self.transfer == self.linear\n",
    "                self.transfer_derivative = selflinear_derivative\n",
    "        else:# Si no se especifica una funcion de transferencia usaremos por defecto la sigmoidal\n",
    "            self.transfer = self.sigmoid\n",
    "            self.transfer_derivative = self.sigmoid_derivative\n",
    "            \n",
    "        #Si es la primera capa no tiene matriz de pesos\n",
    "        if(self.layer_index == 0):\n",
    "            self.weights_matrix = np.identity(self.neurons_in_layer)\n",
    "            #Con esto logramos una matriz identidad que al ser multiplicada por el vector de inputs\n",
    "            #nos da el mismo vector de inputs\n",
    "        else:\n",
    "            self.weights_matrix = np.random.rand(self.neurons_in_layer, neurons_previous_layer)\n",
    "    \n",
    "    def compute_layer_activations(self, activations_prev_layer):\n",
    "        \n",
    "        #Primero calculamos el input_sum, a la capa de input hay que simplemente pasarle la primera fila\n",
    "        #de los datos como 'activations_prev_layer' cuando se entrene la red\n",
    "        self.input_sum = np.dot(self.get_weights_matrix(), activations_prev_layer)\n",
    "        \n",
    "        #Ahora que calculamos el input sum, hay que pasar el resultado por la función de transferencia para ver\n",
    "        #que es lo que las neuronas estan escupiendo\n",
    "        \n",
    "        self.activations_column = [self.transfer(activ_weights_sum) for activ_weights_sum in self.input_sum]\n",
    "        #listo :V\n",
    "        \n",
    "        \n",
    "    ###### Funciones de activacion #########\n",
    "    \n",
    "    # Sigmoid\n",
    "    def sigmoid(self, activation):\n",
    "        TOL = 1e-12 #La sigmoidal se agila en los extremos asi que hay que ponerle un limite de tolerancia\n",
    "        if(activation > 0):\n",
    "            activ = np.maximum(TOL, activation)\n",
    "        elif(activation < 0):\n",
    "            activ = np.maximum(-100,activation)\n",
    "        return 1 / (1 + np.exp(-activ))\n",
    "    def sigmoid_derivative(self, input_sum):\n",
    "        return self.sigmoid(input_sum)(1.0 - self.sigmoid(input_sum))\n",
    "    \n",
    "    # SoftMax\n",
    "    def softmax(self, activation):\n",
    "        return exp(activation)/np.sum(map(exp, self.activations_column))\n",
    "    def softmax_derivative(self, input_sum):\n",
    "        return self.softmax(input_sum)(1.0 - self.softmax(input_sum))\n",
    "        \n",
    "    # Linear\n",
    "    def linear(self, activation):\n",
    "        return activation\n",
    "    def linear_derivative(self, input_sum):\n",
    "        return 1.0\n",
    "    \n",
    "    ########################################\n",
    "    def get_parameters(self):\n",
    "        layer_params = {'neurons_in_layer':self.neurons_in_layer, 'weights_matrix':self.weights_matrix, \n",
    "                        'index':self.layer_index, 'input_sum':self.input_sum, \n",
    "                        'activations_column':self.activations_column}\n",
    "        return layer_params\n",
    "    \n",
    "    def get_weights_matrix(self):\n",
    "        return self.weights_matrix\n",
    "    \n",
    "    def get_index(self):\n",
    "        return self.layer_index\n",
    "    \n",
    "    def get_input_sum(self):\n",
    "        return self.input_sum\n",
    "    \n",
    "    def get_activations_column(self):\n",
    "        return self.activations_column\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mynet = Network(5,(3,2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dict_net =  mynet.get_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = dict_net['hidden_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.5692296])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[3].get_activations_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=layers[2].get_parameters()['weights_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = np.ones((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet.forward_propagate(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44390954])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [lineal(activation) for activation in b]\n",
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
